---
name: test-analyze
description: Analyze gaps between TESTCASE_STATUS.md and actual test files
---

# Test Analyze Skill

Compares the documented test cases (TESTCASE_STATUS.md) with actual test files to find missing tests.

---

## Input

- `target`: The project target (frontend, backend, frontend-dashboard)

---

## Step 1: Read TESTCASE_STATUS.md

Read the status file:
```
.claude-project/status/{target}/TESTCASE_STATUS.md
```

Parse the markdown table to extract:
- Test titles
- URLs
- Status (Complete, In Progress, Not Started, Skipped, Blocked)
- Test code paths

Store as `documented_tests[]`.

---

## Step 2: Scan Actual Test Files

### For Backend (Jest)

Scan: `backend/test/**/*.e2e-spec.ts`

Parse patterns:
```typescript
describe('Category Name', () => {
  it('test case title', async () => { ... });
  it.skip('skipped test', async () => { ... });
});
```

Regex:
- `describe\(['"](.+?)['"]` → Category
- `it\(['"](.+?)['"]` → Test title + line number
- `it\.skip\(['"](.+?)['"]` → Skipped test

### For Frontend / Frontend-Dashboard (Playwright)

Scan: `{target}/test/tests/**/*.spec.ts`

Parse patterns:
```typescript
test.describe('Category Name', () => {
  test('test case title', async ({ page }) => { ... });
  test.skip('skipped test', async ({ page }) => { ... });
});
```

Regex:
- `test\.describe\(['"](.+?)['"]` → Category
- `test\(['"](.+?)['"]` → Test title + line number
- `test\.skip\(['"](.+?)['"]` → Skipped test

Store as `actual_tests[]`.

---

## Step 3: Scan Application Code for Features

### For Backend

Scan controllers: `backend/src/modules/**/**.controller.ts`

Extract endpoints:
```typescript
@Get('path')
@Post('path')
@Patch('path')
@Delete('path')
```

Store as `features[]`.

### For Frontend

Scan routes/pages:
- Route definitions: `{target}/app/routes.tsx` or similar
- Page components: `{target}/app/pages/**/*.tsx`

Extract routes/screens.

Store as `features[]`.

---

## Step 4: Compare and Find Gaps

### Gap Type 1: Missing Tests (in STATUS but not in files)

```
For each test in documented_tests:
  If test.status == "Not Started" AND no matching actual_test:
    Add to missing_tests[]
```

### Gap Type 2: Orphan Tests (in files but not in STATUS)

```
For each test in actual_tests:
  If no matching documented_test:
    Add to orphan_tests[]
```

### Gap Type 3: Untested Features (features without any tests)

```
For each feature in features:
  If no test covers this feature:
    Add to untested_features[]
```

### Gap Type 4: Status Mismatch

```
For each test in documented_tests:
  If test.status == "Not Started" AND matching actual_test exists:
    Add to status_mismatch[]
```

### Gap Type 5: Missing Critical Scenarios

Identify critical tests that should exist but don't:

```
For each documented_test with Memo containing "[Auto]":
  If test.status == "Not Started":
    Add to missing_critical[] with priority HIGH
```

**Critical Scenario Categories (from generate-testcase.md):**

| Category Pattern | Priority | Description |
|------------------|----------|-------------|
| `Auth - Signup` | HIGH | Role registration must work |
| `Auth - Login` | HIGH | Authentication is fundamental |
| `Auth - Logout` | HIGH | Session cleanup is security-critical |
| `Auth - Token Isolation` | HIGH | Cross-panel token leakage is severe |
| `Auth - Authorization` | MEDIUM | Role-based access control |
| `*- CRUD` | MEDIUM | Core functionality |
| `*- Lifecycle` | MEDIUM | Full resource lifecycle |
| `*- Validation` | LOW | Input validation |

---

## Step 4.5: Prioritize Missing Tests

Sort missing tests by priority for actionable output:

```
1. HIGH priority: Auth - Signup, Login, Logout, Token Isolation
2. MEDIUM priority: Authorization, CRUD, Lifecycle
3. LOW priority: Validation, Error handling
4. Other: Feature-specific tests
```

This helps developers focus on critical tests first.

---

## Step 5: Display Analysis Report (Console Only)

Display analysis summary to the user (do NOT save to file):

```markdown
## Test Gap Analysis: {target}

**Analyzed at:** {timestamp}

---

### Summary

| Metric | Count |
|--------|-------|
| Documented Tests | {documented_tests.length} |
| Actual Tests | {actual_tests.length} |
| Untested Features | {untested_features.length} |
| Status Mismatches | {status_mismatch.length} |
| **Missing Critical Scenarios** | {missing_critical.length} |

---

### Missing Critical Scenarios (Priority: HIGH)

| Title | Category | URL | Priority |
|-------|----------|-----|----------|
| {title} | {category} | {url} | HIGH |

*These tests were auto-generated by `/generate-testcase` and should be implemented first.*

---

### Untested Features (Will be added to STATUS)

| Feature | Type | Category |
|---------|------|----------|
| {feature} | {endpoint/screen} | {category} |

---

### Status Mismatches (Will be updated)

| Title | Current Status | Should Be |
|-------|----------------|-----------|
| {title} | Not Started | Pass/Fail |
```

---

## Step 6: Update TESTCASE_STATUS.md Directly

**Instead of creating GAP_ANALYSIS.md, update TESTCASE_STATUS.md directly.**

### 6.1 Add Untested Features as "Not Started"

For each untested feature found in Step 4:

1. Add new row to the test table in `{TARGET_PREFIX}_TESTCASE_STATUS.md`
2. Format:
   ```markdown
   | {feature_title} | {HTTP_METHOD} {endpoint} | Not Started | TBD | {user_type} |
   ```

### 6.2 Update Status Mismatches

For each test marked "Not Started" but code exists:

1. Find the row in TESTCASE_STATUS.md
2. Update status to "Pass" or "Fail" based on last test run
3. Update Test Code column with actual file path

### 6.3 Add Orphan Tests

For each test in code but not documented:

1. Add new row with test info
2. Set status based on last test run
3. Include file path in Test Code column

### File Path

```
.claude-project/status/{target}/{TARGET_PREFIX}_TESTCASE_STATUS.md
```

Example: `.claude-project/status/backend/BACKEND_TESTCASE_STATUS.md`

---

## Output

Returns:
- Analysis summary displayed to console
- `TESTCASE_STATUS.md` updated with:
  - New "Not Started" entries for untested features
  - Updated statuses for mismatched tests
  - Added orphan tests to documentation
- **Missing Critical Scenarios** highlighted with priority
- Summary counts for changes made

### Priority Recommendations

When missing critical scenarios are found:

```markdown
## Recommended Implementation Order

1. **Auth - Signup/Login** (HIGH) - X tests missing
   - {role} signup flow
   - {role} login flow

2. **Auth - Session** (HIGH) - X tests missing
   - Logout cleanup
   - Token isolation

3. **CRUD Flows** (MEDIUM) - X tests missing
   - {resource} lifecycle tests
```

---

## Error Handling

| Error | Action |
|-------|--------|
| TESTCASE_STATUS.md not found | Create new file with untested features |
| No test files found | Report 0 actual tests, add all features as "Not Started" |
| Parse error in spec file | Log warning, continue with partial data |
| Write permission error | Report error, display changes that would be made |
